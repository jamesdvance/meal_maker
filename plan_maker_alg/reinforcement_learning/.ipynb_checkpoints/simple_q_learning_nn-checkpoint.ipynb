{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Q- Learning\n",
    "### Finding Ten Discrete Meals To Fit Calorie Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from meal_plan_env import DayPlan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in nutrition and reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_reqs_df = pd.read_csv(\"C:/Users/J/Fitness/Meal Plans/num_reqs_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calories</th>\n",
       "      <th>protein_g</th>\n",
       "      <th>fat_g</th>\n",
       "      <th>saturated_fat_g</th>\n",
       "      <th>carb_g</th>\n",
       "      <th>fiber_g</th>\n",
       "      <th>sugar_g</th>\n",
       "      <th>sodium_mg</th>\n",
       "      <th>cholesterol_mg</th>\n",
       "      <th>calcium_mg</th>\n",
       "      <th>iron_mg</th>\n",
       "      <th>vit_a_mcg</th>\n",
       "      <th>vit_c_mg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2226.178233</td>\n",
       "      <td>222.617823</td>\n",
       "      <td>86.573598</td>\n",
       "      <td>12.367657</td>\n",
       "      <td>139.13614</td>\n",
       "      <td>31.166495</td>\n",
       "      <td>27.827228</td>\n",
       "      <td>2400</td>\n",
       "      <td>300</td>\n",
       "      <td>1000</td>\n",
       "      <td>18</td>\n",
       "      <td>5000</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      calories   protein_g      fat_g  saturated_fat_g     carb_g    fiber_g  \\\n",
       "0  2226.178233  222.617823  86.573598        12.367657  139.13614  31.166495   \n",
       "\n",
       "     sugar_g  sodium_mg  cholesterol_mg  calcium_mg  iron_mg  vit_a_mcg  \\\n",
       "0  27.827228       2400             300        1000       18       5000   \n",
       "\n",
       "   vit_c_mg  \n",
       "0        60  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_reqs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in partial meal plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "part_pl = pd.read_csv(\"C:/Users/J/Fitness/partial_plan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "part_agg = pd.DataFrame([list(map(lambda x: sum(part_pl[x]),num_reqs_df.columns.values))], columns=num_reqs_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_reqs_df =  num_reqs_df - part_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will look for 1 recipe that best completes this meal plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pl = DayPlan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pl.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pl.set_num_reqs(new_reqs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Set Up\n",
    "\n",
    "Simple - layer Q - Network that learns which food item to pick given 15 or less to get to an optimal calorie count with additional reward for better nutritional value\n",
    "\n",
    "Neural Network: \n",
    "\n",
    "One vector with one-hot encoding for state # for each state that exists. \n",
    "For each above vector, a vector of all 450k Q values\n",
    "\n",
    "The loss function is the difference between the current predicted Q values and the 'target' value\n",
    "So it is trying to get as accurate as possible in predicting the Q values for each state/action\n",
    "\n",
    "Because 15 states x 450K actions is going to be too costly for an example problem, will prove out the example using a partial meal plan: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e43ca4b02940>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;31m# Randomized Weights For Each Action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[1;31m# Matrix Multiply the inputs (states) times the weights (actions)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mQout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "# Reset Tensorflow\n",
    "tf.reset_default_graph()\n",
    "# Shape Tensor - 1 x 10\n",
    "inputs = tf.placeholder(shape =[1,10],dtype=tf.float32)\n",
    "# Randomized Weights For Each Action\n",
    "W = tf.Variable(tf.random_uniform([10, len(pl.df)],0,0.1))\n",
    "# Matrix Multiply the inputs (states) times the weights (actions)\n",
    "Qout = tf.matmul(inputs, W)\n",
    "# Choose index of the largest value in the output accross the 1st axis (row) - choosing the largest expected 'reward'\n",
    "predict = tf.argmax(Qout, 1)\n",
    "# Insert placeholder for action\n",
    "nextQ = tf.placeholder(shape=[1,len(pl.df)], dtype=tf.float32)\n",
    "# Loss function:  Q_target - Q\n",
    "loss = tf.reduce_sum(tf.square(nextQ-Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.identity(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.identity(10)[3:3+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Setting learning parameters\n",
    "y=.99 \n",
    "e =0.1\n",
    "num_episodes = 10000\n",
    "# Lists to capture total rewards per episode\n",
    "rList = []\n",
    "# \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        # Reset environment and randomly sample to return first observation\n",
    "        s = pl.reset()\n",
    "        # Total expected reward value at state\n",
    "        rAll = 0\n",
    "        # because this is feed-forward, can set d to be going over by 75 calories. Can't work backwards, so bust\n",
    "        d = False\n",
    "        # \n",
    "        # Q Network Iteration\n",
    "        j = 0\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "            # 'predict' :\n",
    "            #     take the max of Qout, 1. \n",
    "            # 'Qout':\n",
    "            #     matrix multiply inputs x weights. Weights is a matrix with 1 row per state and and one column per action\n",
    "            #     W holds...\n",
    "            #     \n",
    "            # 'feed_dict={inputs:np.identity(10)[s:s+1]}' :\n",
    "            #     Choose an action greedily from Q-Network. Includes e chance of random action\n",
    "            #     maps the 1 x 10 placeholder to one row of the identity matrix corresponding to the value randomly returned\n",
    "            #     by the reset function. Feed_dict maps the 1x10 graph elements to the identity matrix at row s+1\n",
    "            # 'a'\n",
    "            #    The result of tf evaluating predict: returning the index of the largest element accross the tensor\n",
    "            # 'allQ'\n",
    "            #    The result of tf evaluating Qout: which is the matrix multiplication of input (states) times weights \n",
    "            #    (states x actions)\n",
    "            a, allQ = sess.run([predict, Qout], feed_dict={inputs:np.identity(10)[s:s+1]})\n",
    "            #  try random number between 0 and 1. If it's higher than error threshold -- \n",
    "            if np.random.rand(1) > e:\n",
    "            # \n",
    "            # 'a[0]' \n",
    "            #     holds the index of the highest Q-value along the first row\n",
    "            # 'env.action_space.sample()'\n",
    "            #     return random index randomly sampled from action space\n",
    "                a[0] = env.action_space.sample()\n",
    "                \n",
    "            # 'env.step(a[0])'\n",
    "            #     submits and action (the index of the expected highest Q-value) and returns tuple (observation, reward, done, info)\n",
    "            #     s1: \n",
    "            s1, r, d, _ = env.step(a[0])\n",
    "                \n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01943258])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
