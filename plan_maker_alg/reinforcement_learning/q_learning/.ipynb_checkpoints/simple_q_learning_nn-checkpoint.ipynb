{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Q- Learning\n",
    "### Finding Ten Discrete Meals To Fit Calorie Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from meal_plan_env import DayPlan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in nutrition and reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/J/Desktop/Businesses/Meal_Maker/Scraped_Data/combined_nutrition_small/nutrition_sm_processed_ss.csv',dtype = {'food_key':int,'ingredients_list':object}, encoding='ISO-8859-1')\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_reqs_df = pd.read_csv(\"C:/Users/J/Fitness/Meal Plans/num_reqs_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calories</th>\n",
       "      <th>protein_g</th>\n",
       "      <th>fat_g</th>\n",
       "      <th>saturated_fat_g</th>\n",
       "      <th>carb_g</th>\n",
       "      <th>fiber_g</th>\n",
       "      <th>sugar_g</th>\n",
       "      <th>sodium_mg</th>\n",
       "      <th>cholesterol_mg</th>\n",
       "      <th>calcium_mg</th>\n",
       "      <th>iron_mg</th>\n",
       "      <th>vit_a_mcg</th>\n",
       "      <th>vit_c_mg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2226.178233</td>\n",
       "      <td>222.617823</td>\n",
       "      <td>86.573598</td>\n",
       "      <td>12.367657</td>\n",
       "      <td>139.13614</td>\n",
       "      <td>31.166495</td>\n",
       "      <td>27.827228</td>\n",
       "      <td>2400</td>\n",
       "      <td>300</td>\n",
       "      <td>1000</td>\n",
       "      <td>18</td>\n",
       "      <td>5000</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      calories   protein_g      fat_g  saturated_fat_g     carb_g    fiber_g  \\\n",
       "0  2226.178233  222.617823  86.573598        12.367657  139.13614  31.166495   \n",
       "\n",
       "     sugar_g  sodium_mg  cholesterol_mg  calcium_mg  iron_mg  vit_a_mcg  \\\n",
       "0  27.827228       2400             300        1000       18       5000   \n",
       "\n",
       "   vit_c_mg  \n",
       "0        60  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_reqs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in partial meal plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "part_pl = pd.read_csv(\"C:/Users/J/Fitness/partial_plan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "part_agg = pd.DataFrame([list(map(lambda x: sum(part_pl[x]),num_reqs_df.columns.values))], columns=num_reqs_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_reqs_df =  num_reqs_df - part_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will look for 3 states that best completes this meal plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp_df = df[(df.food_type_grp=='restaurant') & (df.brand=='Sweetgreen')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pl = DayPlan(sp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pl.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pl.set_num_reqs(new_reqs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Set Up\n",
    "\n",
    "Simple - layer Q - Network that learns which food item to pick given 15 or less to get to an optimal calorie count with additional reward for better nutritional value\n",
    "\n",
    "Neural Network: \n",
    "\n",
    "One vector with one-hot encoding for state # for each state that exists. \n",
    "For each above vector, a vector of all 450k Q values\n",
    "\n",
    "The loss function is the difference between the current predicted Q values and the 'target' value\n",
    "So it is trying to get as accurate as possible in predicting the Q values for each state/action\n",
    "\n",
    "Because 15 states x 450K actions is going to be too costly for an example problem, will prove out the example using a partial meal plan: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reset Tensorflow\n",
    "tf.reset_default_graph()\n",
    "# Shape Tensor - 1 x 10\n",
    "inputs = tf.placeholder(shape =[1,10],dtype=tf.float32)\n",
    "# Randomized Weights For Each Action\n",
    "W = tf.Variable(tf.random_uniform([10, len(pl.df)],0,0.1))\n",
    "# Matrix Multiply the inputs (states) times the weights (actions)\n",
    "Qout = tf.matmul(inputs, W)\n",
    "# Choose index of the largest value in the output accross the 1st axis (row) - choosing the largest expected 'reward'\n",
    "predict = tf.argmax(Qout, 1)\n",
    "# Insert placeholder for action\n",
    "nextQ = tf.placeholder(shape=[1,len(pl.df)], dtype=tf.float32)\n",
    "# Loss function:  Q_target - Q\n",
    "loss = tf.reduce_sum(tf.square(nextQ-Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.identity(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.identity(10)[0:0+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Setting learning parameters\n",
    "y=.99 \n",
    "e =0.1 # probability of random pick\n",
    "n = 3 # number of states\n",
    "num_episodes = 10000\n",
    "# Lists to capture total rewards per episode\n",
    "rList = []\n",
    "# \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        # Reset environment\n",
    "        pl.reset()\n",
    "        # Start at state 0\n",
    "        s = 0\n",
    "        # Total expected reward value at state\n",
    "        rAll = 0\n",
    "        # Set 'done' variable\n",
    "        d = False\n",
    "        # \n",
    "        # Q Network Iteration\n",
    "        j = 0\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "            # 'predict' :\n",
    "            #     take the max of Qout, 1. \n",
    "            # 'Qout':\n",
    "            #     matrix multiply inputs x weights. Weights is a matrix with 1 row per state and and one column per action\n",
    "            #     W holds...\n",
    "            #     \n",
    "            # 'feed_dict={inputs:np.identity(10)[s:s+1]}' :\n",
    "            #     Choose an action greedily from Q-Network. Includes e chance of random action\n",
    "            #     maps the 1 x 10 placeholder to one row of the identity matrix corresponding to the value randomly returned\n",
    "            #     by the reset function. Feed_dict maps the 1x10 graph elements to the identity matrix at row s+1\n",
    "            # 'a'\n",
    "            #    The result of tf evaluating predict: returning the index of the largest element accross the tensor\n",
    "            # 'allQ'\n",
    "            #    The result of tf evaluating Qout: which is the matrix multiplication of input (states) times weights \n",
    "            #    (states x actions)\n",
    "            a, allQ = sess.run([predict, Qout], feed_dict={inputs:np.identity(n)[s:s+1]})\n",
    "            #  try random number between 0 and 1. If it's higher than error threshold -- \n",
    "            if np.random.rand(1) > e:\n",
    "            # \n",
    "            # 'a[0]' \n",
    "            #     holds the index of the highest Q-value along the first row. \n",
    "            # 'env.action_space.sample()'\n",
    "            #     return random index randomly sampled from action space\n",
    "                a[0] = env.rand_action\n",
    "                \n",
    "            # 'env.step(a[0])'\n",
    "            #     submits an action (the index of the expected highest Q-value) and returns tuple (observation, reward, done, info)\n",
    "            #     s1: \n",
    "            s1, r, d  = env.step(a[0])\n",
    "            # Inputs times the weights at new state\n",
    "            Q1 = sess.run(Qout,feed_dict={inputs:np.identity(n)[s1, s1+1]}) # state + 1 \n",
    "            # Future expected reward for this next state - this might be a problem for meal planning since future \n",
    "            # Expected reward is dependent on the preceding state\n",
    "            maxQ1 = np.max(Q1) \n",
    "            # Target Q = Q values returned from state s evaluations\n",
    "            targetQ = allQ\n",
    "            targetQ[0, a[0]] = r + y*maxQ1\n",
    "            # This is a problem. The Q - value at a given state for a given action will depend on the previous states\n",
    "            # (meals chosen). Therefore a policy-based agent needs to be used.\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01943258])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
