{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping Different Kinds of Problem Formulations including optimization, predictive, and search-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimization problem might be fastest since each constraint can be added nicely. But what to optimize remains important. With a lot of ratings and with personal ratings, could optimize for 'taste' or 'enjoyment' based on a blend of a star rating for the person and the population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictive model also would benefit a lot from the crowd advantage. But it differs from the optimization in that it would only try to optimize positive clicks. So if taste isn't the biggest factor, it wouldn't recommend based on taste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Reinforcement Learning - it could be modeled with different simultaneous 'agents' controlling to recommendations from each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it can be planned to evolve from manual to AI-based recommendations. But for that to happen, data must be collected correctly and time to viability for each algorithm has to be estimated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting into advanced techniques, will look at the efficacy of the current search-based model on the 450k item dataset. \n",
    "\n",
    "Can simulate partitions and indexes by splitting and sorting dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline simulation will be a single-day meal with the plan to use a restaurant food for one meal only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/jvanc494/Documents/nutrition_sm_ss_formatted.csv\",encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469828"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useful_df = df[pd.notnull(df.calories)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting to make this useable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "useful_df.protein_g[useful_df['protein_g'].isnull()] = 0\n",
    "useful_df.fat_g[useful_df['fat_g'].isnull()] = 0\n",
    "useful_df.carb_g[useful_df['carb_g'].isnull()] = 0\n",
    "#useful_df.isnull().sum()\n",
    "useful_df.protein_g = useful_df.protein_g.astype(int)\n",
    "useful_df.fat_g = useful_df.fat_g.astype(int)\n",
    "useful_df.carb_g = useful_df.carb_g.astype(int)\n",
    "useful_df['prot_prop'] = useful_df.protein_g/(useful_df.protein_g+useful_df.fat_g+useful_df.carb_g)\n",
    "useful_df['fat_prop'] = useful_df.fat_g/(useful_df.protein_g+useful_df.fat_g+useful_df.carb_g)\n",
    "useful_df['carb_prop'] = useful_df.carb_g/(useful_df.protein_g+useful_df.fat_g+useful_df.carb_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting user requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prot_req = 209\n",
    "fat_req = 64\n",
    "carb_req = 183"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Setting group requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food_type_grps = useful_df['food_type_grp'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food_type_grps = sorted(food_type_grps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grocery', 'raw ingredient', 'recipe', 'restaurant']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_type_grps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food_type_grp_vec = {food_type_grps[0]:np.nan,food_type_grps[1]:0,food_type_grps[2]:1,food_type_grps[3]:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grocery': nan, 'raw ingredient': 0, 'recipe': 1, 'restaurant': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_type_grp_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function 'add_sorted_item' takes a point in time of the meal and some mealset (usually a subset) and finds the meals within the mealset that most match the proportions of macros needed (this only works for macros). It returns the updated meal plan, the new delta and the new proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_sorted_item(useful_df, plan_df, delta, proportions):\n",
    "    delta = np.array([prot_req,fat_req,carb_req])-np.array([sum(plan_df['protein_g']),sum(plan_df['fat_g']),sum(plan_df['carb_g'])])\n",
    "    proportions = np.array([delta[0]/sum(delta),delta[1]/sum(delta),delta[2]/sum(delta)])\n",
    "    useful_df['prop_distance'] = np.sqrt((useful_df.prot_prop-proportions[0])**2+(useful_df.fat_prop - proportions[1])**2+(useful_df.carb_prop - proportions[2])**2)# a\n",
    "    useful_df = useful_df.sort_values(by=['prop_distance'])\n",
    "# Append closest distance\n",
    "    plan_df = plan_df.append([useful_df.iloc[0,:]],ignore_index=True)\n",
    "# Drop used\n",
    "    useful_df = useful_df.drop(useful_df.index[[0]])\n",
    "\n",
    "    return plan_df, delta, proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below takes the raw macro requirements and calculates the macros as an array of proportions.\n",
    "Then it takes the distance between the requirement proportions and the macros for each food item to get initial proportions.\n",
    "Then sorts the entire food database by that distance. \n",
    "Then it assigns the first item of the sorted database to the plan. Then it drops that item from the food database\n",
    "Then it re-calculates the delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proportions = np.array([prot_req/sum([prot_req,fat_req,carb_req]),fat_req/sum([prot_req,fat_req,carb_req]),carb_req/sum([prot_req,fat_req,carb_req])])\n",
    "\n",
    "useful_df['prop_distance'] = np.sqrt((useful_df.prot_prop-proportions[0])**2+(useful_df.fat_prop - proportions[1])**2+(useful_df.carb_prop - proportions[2])**2)\n",
    "\n",
    "useful_df = useful_df.sort_values(by=['prop_distance'])\n",
    "plan_df = pd.DataFrame([useful_df.iloc[0,:]])\n",
    "useful_df = useful_df.drop(useful_df.index[[0]])\n",
    "delta = np.array([prot_req,fat_req,carb_req])-np.array([sum(plan_df['protein_g']),sum(plan_df['fat_g']),sum(plan_df['carb_g'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My food_type_grp requirement is to have a recipe for one meal, a restaurant for another meal, and am ambiguous for all others. \n",
    "To accomplish this, am creating seperate breakfast, lunch, dinner and snacks 'meal_df' s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pretty standard requirement would be to not have all the calories take place in one meal. So will start out with some requirements on *calories* per meal. \n",
    "\n",
    "It's important that macros aren't necessarily constrained by meal (especially since further requirements will make this very hard, and it will also remove all possibility of having a little piece of chocolate as a snack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['food_key','meal_time']\n",
    "plan_key_df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe represents one full day, but is split into meal times. \n",
    "This same framework will be useful when abstracting to weekly requirements (veges 3x a week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food_key</th>\n",
       "      <th>meal_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [food_key, meal_time]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan_key_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't know ahead of time how many food items will occur per meal. So will start out with a vector of meal times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meal_times = ['breakfast', 'lunch', 'dinner', 'snack']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want each meal to encompass an appropriate amount of calories, so we'll first calculate the calories reqs from macros and then apply the proportion as a constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cal_req = prot_req*4 + fat_req*9 + carb_req*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2144"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meal_cals = {meal_times[0]:int(cal_req*.2), meal_times[1]:int(cal_req*.3), meal_times[2]:int(cal_req*.4), meal_times[3]:int(cal_req*.1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has to flexible, but a guideline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2142"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(meal_cals.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm has to create a daily meal plan that fits the macro requirements. \n",
    "It does not have to achieve this at the meal time level. In fact, the less it only spits out one type of food, the better. \n",
    "\n",
    "The algorithm below has only one action: select the item from the given list that has the best fit for the requirements. To start with it, it can be given another option of selecting the correct food type for the particular meal time. \n",
    "\n",
    "In order to 'choose' correctly, a Markovian process should occur where the program is aware of the distance to the final requirements and tries to close that distance. But when an item is significantly better in the 'yummy' category, it works to incorporate that one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while sum((abs(delta)<np.array([0.05*prot_req,0.05*fat_req,0.05*carb_req])))!=3:\n",
    "    if sum(delta<0)==3:\n",
    "        plan_df = plan_df.drop(plan_df.index[[randint(0,len(plan_df)-1)]])\n",
    "    if j <= 4:\n",
    "        plan_df, delta, proportions = add_sorted_item(useful_df[useful_df['rand_group']==j], plan_df, delta, proportions)\n",
    "    else:\n",
    "        plan_df, delta, proportions = add_sorted_item(useful_df, plan_df, delta, proportions)\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'breakfast': 428, 'dinner': 857, 'lunch': 643, 'snack': 214}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meal_cals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([119,  53, 109], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.45833333,  0.14035088,  0.40131579])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meal_times_food_types = {'breakfast':'grocery_item', 'lunch':'restaurant', 'dinner':'recipe', 'snack':'grocery_item'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until there is any data on 'likes', the problem is missing that dimension. In this case we assume there is no other good way to select a food other than 'appropriate group' and proportions / macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defaults to prop fit given good enough macro reqs\n",
    "def add_prop_fit(useful_df, plan_df, proportions, cal_delta):\n",
    "    #\n",
    "    eval_df = useful_df[['food_key','calories', 'prot_prop','carb_prop','fat_prop']]\n",
    "    #\n",
    "    eval_df['cal_distance'] =  np.sqrt((eval_df.calories - cal_delta)**2)\n",
    "    # Doesn't have to be quantile - can just be some percentage close to zero\n",
    "    eval_df['macro_distance'] = np.sqrt((eval_df.prot_prop-proportions[0])**2+(eval_df.fat_prop - proportions[1])**2+(eval_df.carb_prop - proportions[2])**2) \n",
    "    #\n",
    "    # Could bin the distances in groups of ntile(20), and then pull the best proportions from those groups\n",
    "    # Would be nice if it had some sort of random process at the start that got narrower and narrower towards reqs.\n",
    "    eval_df['cal_ntile'] = pd.qcut(eval_df.cal_distance, q=30)\n",
    "    eval_df = eval_df.sort_values(by=['macro_distance'])\n",
    "    # Could we select the top n % by a random distribution favoring the top, \n",
    "# Append closest distance\n",
    "    plan_df = plan_df.append([eval_df.iloc[0,:]],ignore_index=True)\n",
    "    cal_delta = sum(plan_df.calories) - cal_delta\n",
    "    prot = sum(plan_df.protein_g)\n",
    "    fat = sum(plan_df.fat_g)\n",
    "    carb = sum(plan_df.carb_g)\n",
    "    \n",
    "    proportions = np.array([prot/(prot+fat+carb),fat/(prot+fat+carb),carb/(prot+fat+carb)])\n",
    "\n",
    "    return plan_df, cal_delta, proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['food_key', 'food_description', 'brand', 'food_type_grp', 'source',\n",
       "       'ingredients_list', 'serving_size_raw', 'serving_size_val',\n",
       "       'serving_size_unit', 'calories', 'protein_g', 'fat_g',\n",
       "       'saturated_fat_g', 'carb_g', 'fiber_g', 'sugar_g', 'sodium_mg',\n",
       "       'cholesterol_mg', 'calcium_mg', 'iron_mg', 'vit_a_mcg', 'vit_c_mg',\n",
       "       'prot_prop', 'fat_prop', 'carb_prop', 'prop_distance'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Bin edges must be unique: array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]).\nYou can drop duplicate edges by setting the 'duplicates' kwarg",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-90fe227f67db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# This level of granularity doesn't have to be for meal types. Can keep going smaller to courses, sides, etc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmeal_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmeal_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcal_delta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproportions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_prop_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0museful_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0museful_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfood_type_grp\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mfood_type_grp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeal_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproportions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcal_delta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcal_delta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcalories\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-f9847242ad78>\u001b[0m in \u001b[0;36madd_prop_fit\u001b[1;34m(useful_df, plan_df, proportions, cal_delta)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Could bin the distances in groups of ntile(20), and then pull the best proportions from those groups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Would be nice if it had some sort of random process at the start that got narrower and narrower towards reqs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0meval_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cal_ntile'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcal_distance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0meval_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'macro_distance'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Could we select the top n % by a random distribution favoring the top,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\tile.py\u001b[0m in \u001b[0;36mqcut\u001b[1;34m(x, q, labels, retbins, precision, duplicates)\u001b[0m\n\u001b[0;32m    204\u001b[0m     fac, bins = _bins_to_cuts(x, bins, labels=labels,\n\u001b[0;32m    205\u001b[0m                               \u001b[0mprecision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_lowest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m                               dtype=dtype, duplicates=duplicates)\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     return _postprocess_for_cut(fac, bins, retbins, x_is_series,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\tile.py\u001b[0m in \u001b[0;36m_bins_to_cuts\u001b[1;34m(x, bins, right, labels, precision, include_lowest, dtype, duplicates)\u001b[0m\n\u001b[0;32m    230\u001b[0m             raise ValueError(\"Bin edges must be unique: {}.\\nYou \"\n\u001b[0;32m    231\u001b[0m                              \u001b[1;34m\"can drop duplicate edges by setting \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m                              \"the 'duplicates' kwarg\".format(repr(bins)))\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mbins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_bins\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Bin edges must be unique: array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]).\nYou can drop duplicate edges by setting the 'duplicates' kwarg"
     ]
    }
   ],
   "source": [
    "plan_df = pd.DataFrame()\n",
    "for meal in meal_times:\n",
    "    calories = meal_cals[meal]\n",
    "    cal_delta = calories\n",
    "    food_type_grp = meal_times_food_types[meal]\n",
    "    ###### Add Other Constraints on Meal Group Here - including preferences and product mix \n",
    "    # This level of granularity doesn't have to be for meal types. Can keep going smaller to courses, sides, etc\n",
    "    meal_df = pd.DataFrame()\n",
    "    meal_df, cal_delta, proportions = add_prop_fit(useful_df[useful_df.food_type_grp==food_type_grp], meal_df,proportions, cal_delta)\n",
    "    \n",
    "    while abs(cal_delta) >= 0.1*calories:\n",
    "        meal_df, cal_delta, proportions = add_prop_fit(useful_df[useful_df.food_type_grp==food_type_grp], meal_df,proportions, cal_delta)\n",
    "    \n",
    "    plan_df = plan_df.append([meal_df],ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185754     521.0\n",
       "351227     170.0\n",
       "383894     170.0\n",
       "138695     170.0\n",
       "116103     160.0\n",
       "1474       250.0\n",
       "188998     260.0\n",
       "170505     540.0\n",
       "122147     460.0\n",
       "170438    1266.0\n",
       "Name: calories, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_df.calories[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'snack'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The edit and replace process will also be a very important function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
